{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "genuine-tragedy",
   "metadata": {},
   "source": [
    "# Neural network based integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nasty-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from dynamical_system import *\n",
    "from time_integrator import *\n",
    "from models import *\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2512517)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-mattress",
   "metadata": {},
   "source": [
    "## Neural network integrators\n",
    "Let $q(t)=(x(t),v(t))\\in\\mathbb{R}^{2d}$ be the state vector. Two methods are implemented to advance this state vector in time:\n",
    "\n",
    "### Multistep neural network\n",
    "The `MultistepNNInetgrator` class is used to implement a neural network based integrator with a $S$-step method. This is integrating a given $d$-dimensional system $\\frac{dq(t)}{dt}=\\mathcal{N}(q(t))$. The underlying neural model implements the mapping\n",
    "\n",
    "$$\n",
    "q^{(t-(S-1)\\Delta t)},\\dots,q^{(t-\\Delta t)},q^{(t)} \\mapsto q^{(t+\\Delta t)}\n",
    "$$\n",
    "\n",
    "Internally this is realised by mapping the $B\\times S \\times d$ tensor $X$ to the $B\\times d$ tensor $y$, where $B$ is the minibatch-size. This mapping is of the following form:\n",
    "\n",
    "$$\n",
    "y_{b,j} = X_{b,S-1,j} + \\Delta t \\cdot \\Phi_{bj}(X)\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is a dense neural network. Note that for each batch index $b$, $X_{b,S-1,\\cdot}$ is simply the vector $q^{(t)}_b$, i.e. we assume that $q^{(t+\\Delta t)}$ is $q^{(t)}$ plus $\\Delta t$ times some correction. The neural network $\\Phi$ can take different form:\n",
    "* it can simply be a set of dense layers or\n",
    "* it can be a two-layer LSTM network, followed by a dense layer as in [https://arxiv.org/abs/2004.06493](https://arxiv.org/abs/2004.06493)\n",
    "\n",
    "### Hamiltonian neural network integrator\n",
    "Alternatively, the `HamiltonianNNIntegrator` implements a single-step Stoermer-Verlet method for a Hamiltonian system, following the ideas in [https://arxiv.org/abs/1906.01563](https://arxiv.org/abs/1906.01563). In this case the update $q^{(t)}\\mapsto q^{(t+\\Delta t)}$ takes the form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^{(t+\\Delta t/2)} &= v^{(t)} - \\frac{\\Delta t}{2} \\frac{\\partial V}{\\partial x}\\left(x^{(t)}\\right)\\\\[1ex]\n",
    "x^{(t+\\Delta t)} &= x^{(t)} + \\Delta t \\frac{\\partial T}{\\partial v}\\left(v^{(t+\\Delta t/2)}\\right)\\\\[1ex]\n",
    "v^{(t+\\Delta t)} &= v^{(t)} - \\frac{\\Delta t}{2} \\frac{\\partial V}{\\partial x}\\left(x^{(t+\\Delta t)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Assuming that the Hamiltonian $H(x,v) = T(v) + V(x)$ is separable, the kinetic energy $T(v)$ and potential energy $V(x)$ are represented by neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "animated-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNIntegrator(object):\n",
    "    '''Base class for neural network based integrators\n",
    "    \n",
    "    :arg dynamical_system: dynamical system used for integration\n",
    "    :arg dt: timestep size\n",
    "    :arg nsteps: number of multisteps\n",
    "    '''\n",
    "    def __init__(self,dynamical_system,dt,nsteps):\n",
    "        self.dynamical_system = dynamical_system\n",
    "        self.dt = dt        \n",
    "        self.dim = 2*self.dynamical_system.dim\n",
    "        self.nsteps = nsteps\n",
    "        self.xv = np.zeros((1,self.nsteps,self.dim))\n",
    "\n",
    "    def set_state(self,x,v):\n",
    "        '''Set the current state of the integrator\n",
    "        \n",
    "        :arg x: Array of size nsteps x dim with initial positions\n",
    "        :arg v: Array of size nsteps x dim with initial velocities\n",
    "        '''\n",
    "        self.xv[0,:,:self.dim//2] = x[:,:]\n",
    "        self.xv[0,:,self.dim//2:] = v[:,:]\n",
    "        \n",
    "    @property\n",
    "    def x(self):\n",
    "        '''Return the current position vector (as a d-dimensional array)'''\n",
    "        return self.xv[0,-1,:self.dim//2]\n",
    "\n",
    "    @property\n",
    "    def v(self):\n",
    "        '''Return the current velocity vector (as a d-dimensional array)'''\n",
    "        return self.xv[0,-1,self.dim//2:]\n",
    "    \n",
    "    def integrate(self,n_steps):\n",
    "        '''Carry out a given number of integration steps\n",
    "        \n",
    "        :arg n_steps: number of integration steps\n",
    "        '''\n",
    "        for k in range(n_steps):\n",
    "            x_pred = np.asarray(self.model.predict(self.xv)).flatten()\n",
    "            self.xv = np.roll(self.xv, -1, axis=1)\n",
    "            self.xv[0,-1,:] = x_pred[:]\n",
    "            \n",
    "    def energy(self):\n",
    "        return self.dynamical_system.energy(self.x,self.v)\n",
    "\n",
    "class MultistepNNIntegrator(NNIntegrator):\n",
    "    '''Multistep integrator. Use a neural network to predict the next state, given\n",
    "    a number of previous states\n",
    "    \n",
    "    :arg dynamical_system: dynamical system used for integration\n",
    "    :arg dt: timestep size\n",
    "    :arg nsteps: Number of steps of the timestepping method\n",
    "    :arg dense_layers: neural network layers used to predict the next state\n",
    "    '''\n",
    "    def __init__(self,dynamical_system,dt,nsteps,dense_layers):\n",
    "        super().__init__(dynamical_system,dt,nsteps)\n",
    "        self.dim = 2*self.dynamical_system.dim\n",
    "        self.dense_layers = dense_layers\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        inputs = keras.Input(shape=(self.nsteps,self.dim))\n",
    "        q_n = tf.unstack(inputs,axis=1)[-1]        \n",
    "        output_layer = keras.layers.Dense(self.dim)\n",
    "        x = inputs\n",
    "        for layer in dense_layers:\n",
    "            x = layer(x)\n",
    "        x = output_layer(x)\n",
    "        x = keras.layers.Rescaling(self.dt)(x)\n",
    "        outputs = keras.layers.Add()([q_n,x])\n",
    "        self.model = keras.Model(inputs=inputs,outputs=outputs)\n",
    "        self.model.compile(loss='mse',metrics=[],optimizer=keras.optimizers.Adam(learning_rate=1.E-4))\n",
    "        self.xv = np.zeros((1,self.nsteps,self.dim))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        '''Evaluate model\n",
    "        \n",
    "        Split the inputs = (q_n,p_n) into position and momentum and \n",
    "        return the state (q_{n+1},p_{n+1}) at the next timestep.\n",
    "        \n",
    "        Note that the expected tensor shape is B x 1 x 2d to be compatible with\n",
    "        the non-symplectic update \n",
    "        \n",
    "        :arg inputs: state (q_n,p_n) as a B x 1 x 2d tensor\n",
    "        '''\n",
    "        \n",
    "        input_shape = tf.shape(inputs)\n",
    "        # Extract q_n and p_n from input\n",
    "        qp_old = tf.unstack(tf.reshape(inputs, (input_shape[0],input_shape[2],)),axis=-1)\n",
    "        q_old = tf.stack(qp_old[:self.dim//2],axis=-1)\n",
    "        p_old = tf.stack(qp_old[self.dim//2:],axis=-1)\n",
    "        q_new, p_new = self.verlet_step(q_old,p_old)        \n",
    "        # Combine result of Verlet step into tensor of correct size\n",
    "        outputs = tf.stack([q_new,p_new],axis=-1)        \n",
    "        return outputs\n",
    "\n",
    "class HamiltonianNNIntegrator(NNIntegrator):\n",
    "    '''Neural network integrator based on the Hamiltonian Stoermer-Verlet update'''\n",
    "    def __init__(self,dynamical_system,dt,V_pot_layers,T_kin_layers):\n",
    "        super().__init__(dynamical_system,dt,1)\n",
    "        self.V_pot_layers = V_pot_layers\n",
    "        self.T_kin_layers = T_kin_layers\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        self.model = VerletModel(self.dim,self.dt,\n",
    "                                 self.V_pot_layers,\n",
    "                                 self.T_kin_layers)\n",
    "        self.model.build(input_shape=(None,1,self.dim))\n",
    "        self.model.compile(loss='mse',metrics=[],optimizer=keras.optimizers.Adam(learning_rate=1.E-4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-shareware",
   "metadata": {},
   "source": [
    "### Set up system\n",
    "Set system parameters, construct dynamical system and integrator.\n",
    "\n",
    "The model system we are using here is the harmonic oscillator, defined by the equations of motion\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = v,\\qquad\\qquad\n",
    "\\frac{dv}{dt} = -\\frac{k}{m}x\n",
    "$$\n",
    "\n",
    "The timestep size of the Neural network integrator is set to $\\Delta t=40\\Delta t_{\\text{Verlet}}$ where $\\Delta t_{\\text{Verlet}}$ is the step size of the Verlet integrator that is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"verlet_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  32        \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  272       \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  272       \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  32        \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  272       \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  17        \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,186\n",
      "Trainable params: 1,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mass of particle\n",
    "mass = 1.2\n",
    "# spring constant of harmonic oscillator\n",
    "k_spring = 0.9\n",
    "# timestep for Verlet integrator\n",
    "dt_verlet = 0.005\n",
    "# timestep for neural network integrator\n",
    "dt = 0.2\n",
    "# number of steps for multistep neural network integrator\n",
    "nsteps = 6\n",
    "\n",
    "# use Hamiltonian model?\n",
    "use_hamiltonian = True\n",
    "# use LSTM network for multistep integrator?\n",
    "use_LSTM = True\n",
    "\n",
    "# dynamical system to integrate\n",
    "harmonic_oscillator = HarmonicOscillator(mass,k_spring)\n",
    "# Verlet integrator used to generate data\n",
    "verlet_integrator = VerletIntegrator(harmonic_oscillator,dt_verlet)\n",
    "\n",
    "if use_hamiltonian:\n",
    "    V_pot_layers = [keras.layers.Dense(16,activation='sigmoid'),\n",
    "                    keras.layers.Dense(16,activation='sigmoid'),\n",
    "                    keras.layers.Dense(16,activation='sigmoid')]\n",
    "    T_kin_layers = [keras.layers.Dense(16,activation='sigmoid'),\n",
    "                    keras.layers.Dense(16,activation='sigmoid'),\n",
    "                    keras.layers.Dense(16,activation='sigmoid')]\n",
    "    nn_integrator = HamiltonianNNIntegrator(harmonic_oscillator,dt,V_pot_layers,T_kin_layers)    \n",
    "else:\n",
    "    if use_LSTM: \n",
    "        # Use two layers of LSTMs followed by a dense layer\n",
    "        dense_layers = [keras.layers.LSTM(64,return_sequences=True),\n",
    "                        keras.layers.LSTM(64),\n",
    "                        keras.layers.Dense(32,activation='tanh')]\n",
    "    else:\n",
    "        # Just use several dense layers\n",
    "        dense_layers = [keras.layers.Flatten(),\n",
    "                        keras.layers.Dense(32,activation='tanh'),\n",
    "                        keras.layers.Dense(64,activation='tanh'),\n",
    "                        keras.layers.Dense(32,activation='tanh')]\n",
    "\n",
    "\n",
    "    nn_integrator = MultistepNNIntegrator(harmonic_oscillator,nsteps,dt,\n",
    "                                          dense_layers)\n",
    "\n",
    "# visualise the neural network model\n",
    "nn_integrator.model.summary()\n",
    "#keras.utils.plot_model(nn_integrator.model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-pledge",
   "metadata": {},
   "source": [
    "## Data generator\n",
    "The following data generator class can be used to construct training data samples of the form $(X_j,y_j)$ where\n",
    "\n",
    "$$\n",
    "X_j = q_j^{(0)},q_j^{(\\Delta t)},\\dots,q_j^{((S-1)\\Delta t)},\\qquad\\qquad y_j = q_j^{(S\\Delta t)}.\n",
    "$$\n",
    "\n",
    "Here $q_j^{(0)}$ is a randomly chosen initial condition and the states $q_j^{(\\Delta t)},q_j^{(2\\Delta t)},\\dots,q_j^{((S-1)\\Delta t)}, q_j^{(S\\Delta t)}$ are generated with a training generator (=Verlet) that is run with a smaller timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "physical-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    def __init__(self,nn_integrator,train_integrator):        \n",
    "        self.nn_integrator = nn_integrator\n",
    "        self.train_integrator = train_integrator\n",
    "        self.dynamical_system = self.nn_integrator.dynamical_system\n",
    "        self.dataset = tf.data.Dataset.from_generator(self._generator,                                                      \n",
    "                                                      output_signature=(\n",
    "                                                          tf.TensorSpec(shape=(self.nn_integrator.nsteps,\n",
    "                                                                               2*self.dynamical_system.dim), dtype=tf.float32),\n",
    "                                                          tf.TensorSpec(shape=(2*self.dynamical_system.dim), dtype=tf.float32)\n",
    "                                                      ))\n",
    "    \n",
    "    def _generator(self):\n",
    "        state = np.zeros((self.nn_integrator.nsteps+1,2*self.dynamical_system.dim))\n",
    "        while True:\n",
    "            self.dynamical_system.set_random_state(state[0,:self.dynamical_system.dim],\n",
    "                                                   state[0,self.dynamical_system.dim:])\n",
    "            self.train_integrator.set_state(state[0,:self.dynamical_system.dim],\n",
    "                                            state[0,self.dynamical_system.dim:])\n",
    "            for k in range(self.nn_integrator.nsteps):\n",
    "                self.train_integrator.integrate(int(self.nn_integrator.dt/self.train_integrator.dt))\n",
    "                state[k+1,:self.dynamical_system.dim] = self.train_integrator.x[:]\n",
    "                state[k+1,self.dynamical_system.dim:] = self.train_integrator.v[:]\n",
    "            X = state[:-1,:]\n",
    "            y = state[-1,:]\n",
    "            yield (X,y)\n",
    "    \n",
    "BATCH_SIZE=64\n",
    "data_generator = DataGenerator(nn_integrator,verlet_integrator)\n",
    "train_batches = data_generator.dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-payroll",
   "metadata": {},
   "source": [
    "## Train neural network based integrator\n",
    "\n",
    "Note that training history can be visualised with tensorboard:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./tb_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bottom-health",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/bias:0', 'dense_7/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_6/bias:0', 'dense_7/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "100/100 [==============================] - ETA: 1:27 - loss: 1.675 - ETA: 4s - loss: 1.9079  - ETA: 4s - loss: 1.989 - ETA: 4s - loss: 2.044 - ETA: 3s - loss: 2.071 - ETA: 3s - loss: 2.057 - ETA: 3s - loss: 2.052 - ETA: 3s - loss: 2.042 - ETA: 3s - loss: 2.028 - ETA: 3s - loss: 1.965 - ETA: 3s - loss: 1.950 - ETA: 3s - loss: 1.943 - ETA: 3s - loss: 1.938 - ETA: 2s - loss: 1.935 - ETA: 2s - loss: 1.939 - ETA: 2s - loss: 1.943 - ETA: 2s - loss: 1.939 - ETA: 2s - loss: 1.941 - ETA: 2s - loss: 1.933 - ETA: 2s - loss: 1.938 - ETA: 2s - loss: 1.927 - ETA: 2s - loss: 1.925 - ETA: 2s - loss: 1.932 - ETA: 2s - loss: 1.926 - ETA: 2s - loss: 1.923 - ETA: 1s - loss: 1.920 - ETA: 1s - loss: 1.914 - ETA: 1s - loss: 1.915 - ETA: 1s - loss: 1.915 - ETA: 1s - loss: 1.913 - ETA: 1s - loss: 1.909 - ETA: 1s - loss: 1.919 - ETA: 1s - loss: 1.908 - ETA: 1s - loss: 1.907 - ETA: 1s - loss: 1.910 - ETA: 1s - loss: 1.919 - ETA: 1s - loss: 1.919 - ETA: 0s - loss: 1.924 - ETA: 0s - loss: 1.928 - ETA: 0s - loss: 1.932 - ETA: 0s - loss: 1.930 - ETA: 0s - loss: 1.929 - ETA: 0s - loss: 1.931 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.930 - ETA: 0s - loss: 1.927 - ETA: 0s - loss: 1.927 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.929 - 5s 39ms/step - loss: 1.9309\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 2.110 - ETA: 3s - loss: 2.047 - ETA: 3s - loss: 2.021 - ETA: 3s - loss: 2.102 - ETA: 3s - loss: 2.043 - ETA: 3s - loss: 2.011 - ETA: 3s - loss: 2.053 - ETA: 3s - loss: 1.982 - ETA: 3s - loss: 2.006 - ETA: 2s - loss: 1.978 - ETA: 2s - loss: 1.988 - ETA: 2s - loss: 2.005 - ETA: 2s - loss: 2.006 - ETA: 2s - loss: 1.984 - ETA: 2s - loss: 1.993 - ETA: 2s - loss: 1.999 - ETA: 2s - loss: 2.009 - ETA: 2s - loss: 1.996 - ETA: 2s - loss: 1.988 - ETA: 2s - loss: 1.996 - ETA: 2s - loss: 1.997 - ETA: 2s - loss: 1.991 - ETA: 2s - loss: 1.997 - ETA: 1s - loss: 1.982 - ETA: 1s - loss: 1.980 - ETA: 1s - loss: 1.987 - ETA: 1s - loss: 1.997 - ETA: 1s - loss: 1.999 - ETA: 1s - loss: 2.003 - ETA: 1s - loss: 2.007 - ETA: 1s - loss: 2.014 - ETA: 1s - loss: 2.019 - ETA: 1s - loss: 2.018 - ETA: 1s - loss: 2.022 - ETA: 1s - loss: 2.017 - ETA: 1s - loss: 2.027 - ETA: 1s - loss: 2.030 - ETA: 0s - loss: 2.024 - ETA: 0s - loss: 2.022 - ETA: 0s - loss: 2.017 - ETA: 0s - loss: 2.011 - ETA: 0s - loss: 2.013 - ETA: 0s - loss: 2.010 - ETA: 0s - loss: 2.009 - ETA: 0s - loss: 2.011 - ETA: 0s - loss: 2.008 - ETA: 0s - loss: 2.006 - ETA: 0s - loss: 2.014 - ETA: 0s - loss: 2.012 - ETA: 0s - loss: 2.011 - 4s 38ms/step - loss: 2.0095\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 2.409 - ETA: 4s - loss: 2.382 - ETA: 3s - loss: 2.219 - ETA: 3s - loss: 2.147 - ETA: 3s - loss: 2.106 - ETA: 3s - loss: 2.089 - ETA: 3s - loss: 2.087 - ETA: 3s - loss: 2.075 - ETA: 3s - loss: 2.077 - ETA: 3s - loss: 2.068 - ETA: 3s - loss: 2.087 - ETA: 3s - loss: 2.075 - ETA: 2s - loss: 2.091 - ETA: 2s - loss: 2.069 - ETA: 2s - loss: 2.062 - ETA: 2s - loss: 2.049 - ETA: 2s - loss: 2.056 - ETA: 2s - loss: 2.059 - ETA: 2s - loss: 2.068 - ETA: 2s - loss: 2.060 - ETA: 2s - loss: 2.062 - ETA: 2s - loss: 2.049 - ETA: 2s - loss: 2.037 - ETA: 2s - loss: 2.026 - ETA: 2s - loss: 2.038 - ETA: 1s - loss: 2.025 - ETA: 1s - loss: 2.019 - ETA: 1s - loss: 2.018 - ETA: 1s - loss: 2.025 - ETA: 1s - loss: 2.021 - ETA: 1s - loss: 2.013 - ETA: 1s - loss: 2.011 - ETA: 1s - loss: 2.012 - ETA: 1s - loss: 2.020 - ETA: 1s - loss: 2.028 - ETA: 1s - loss: 2.032 - ETA: 1s - loss: 2.032 - ETA: 0s - loss: 2.029 - ETA: 0s - loss: 2.031 - ETA: 0s - loss: 2.025 - ETA: 0s - loss: 2.021 - ETA: 0s - loss: 2.028 - ETA: 0s - loss: 2.028 - ETA: 0s - loss: 2.028 - ETA: 0s - loss: 2.023 - ETA: 0s - loss: 2.018 - ETA: 0s - loss: 2.017 - ETA: 0s - loss: 2.020 - ETA: 0s - loss: 2.020 - ETA: 0s - loss: 2.020 - 4s 39ms/step - loss: 2.0147\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 1.835 - ETA: 4s - loss: 1.940 - ETA: 3s - loss: 1.978 - ETA: 3s - loss: 2.039 - ETA: 3s - loss: 2.045 - ETA: 3s - loss: 2.014 - ETA: 3s - loss: 1.989 - ETA: 3s - loss: 1.951 - ETA: 3s - loss: 1.974 - ETA: 3s - loss: 1.998 - ETA: 3s - loss: 2.024 - ETA: 3s - loss: 2.017 - ETA: 3s - loss: 2.023 - ETA: 2s - loss: 2.033 - ETA: 2s - loss: 2.031 - ETA: 2s - loss: 2.045 - ETA: 2s - loss: 2.030 - ETA: 2s - loss: 2.038 - ETA: 2s - loss: 2.028 - ETA: 2s - loss: 2.027 - ETA: 2s - loss: 2.032 - ETA: 2s - loss: 2.030 - ETA: 2s - loss: 2.019 - ETA: 2s - loss: 2.028 - ETA: 2s - loss: 2.038 - ETA: 1s - loss: 2.033 - ETA: 1s - loss: 2.033 - ETA: 1s - loss: 2.032 - ETA: 1s - loss: 2.029 - ETA: 1s - loss: 2.022 - ETA: 1s - loss: 2.008 - ETA: 1s - loss: 2.010 - ETA: 1s - loss: 2.007 - ETA: 1s - loss: 2.016 - ETA: 1s - loss: 2.018 - ETA: 1s - loss: 2.013 - ETA: 1s - loss: 2.019 - ETA: 0s - loss: 2.024 - ETA: 0s - loss: 2.020 - ETA: 0s - loss: 2.024 - ETA: 0s - loss: 2.020 - ETA: 0s - loss: 2.018 - ETA: 0s - loss: 2.016 - ETA: 0s - loss: 2.009 - ETA: 0s - loss: 2.015 - ETA: 0s - loss: 2.015 - ETA: 0s - loss: 2.010 - ETA: 0s - loss: 2.007 - ETA: 0s - loss: 2.000 - ETA: 0s - loss: 1.999 - 4s 39ms/step - loss: 2.0009\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 1.867 - ETA: 3s - loss: 1.751 - ETA: 3s - loss: 1.767 - ETA: 3s - loss: 1.793 - ETA: 3s - loss: 1.821 - ETA: 3s - loss: 1.818 - ETA: 3s - loss: 1.824 - ETA: 3s - loss: 1.828 - ETA: 3s - loss: 1.845 - ETA: 2s - loss: 1.857 - ETA: 2s - loss: 1.826 - ETA: 2s - loss: 1.847 - ETA: 2s - loss: 1.844 - ETA: 2s - loss: 1.864 - ETA: 2s - loss: 1.878 - ETA: 2s - loss: 1.885 - ETA: 2s - loss: 1.885 - ETA: 2s - loss: 1.897 - ETA: 2s - loss: 1.901 - ETA: 2s - loss: 1.919 - ETA: 2s - loss: 1.915 - ETA: 2s - loss: 1.928 - ETA: 2s - loss: 1.929 - ETA: 1s - loss: 1.923 - ETA: 1s - loss: 1.927 - ETA: 1s - loss: 1.918 - ETA: 1s - loss: 1.920 - ETA: 1s - loss: 1.925 - ETA: 1s - loss: 1.930 - ETA: 1s - loss: 1.925 - ETA: 1s - loss: 1.922 - ETA: 1s - loss: 1.913 - ETA: 1s - loss: 1.906 - ETA: 1s - loss: 1.910 - ETA: 1s - loss: 1.909 - ETA: 1s - loss: 1.908 - ETA: 1s - loss: 1.910 - ETA: 0s - loss: 1.905 - ETA: 0s - loss: 1.912 - ETA: 0s - loss: 1.912 - ETA: 0s - loss: 1.913 - ETA: 0s - loss: 1.919 - ETA: 0s - loss: 1.920 - ETA: 0s - loss: 1.919 - ETA: 0s - loss: 1.924 - ETA: 0s - loss: 1.916 - ETA: 0s - loss: 1.912 - ETA: 0s - loss: 1.913 - ETA: 0s - loss: 1.915 - ETA: 0s - loss: 1.913 - 4s 37ms/step - loss: 1.9145\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 2.394 - ETA: 3s - loss: 2.057 - ETA: 3s - loss: 2.017 - ETA: 3s - loss: 1.995 - ETA: 3s - loss: 2.073 - ETA: 3s - loss: 2.022 - ETA: 3s - loss: 2.047 - ETA: 3s - loss: 2.054 - ETA: 3s - loss: 2.014 - ETA: 3s - loss: 2.063 - ETA: 3s - loss: 2.046 - ETA: 3s - loss: 2.075 - ETA: 2s - loss: 2.073 - ETA: 2s - loss: 2.057 - ETA: 2s - loss: 2.038 - ETA: 2s - loss: 2.034 - ETA: 2s - loss: 2.021 - ETA: 2s - loss: 2.017 - ETA: 2s - loss: 2.006 - ETA: 2s - loss: 1.992 - ETA: 2s - loss: 1.979 - ETA: 2s - loss: 1.963 - ETA: 2s - loss: 1.955 - ETA: 2s - loss: 1.950 - ETA: 1s - loss: 1.959 - ETA: 1s - loss: 1.958 - ETA: 1s - loss: 1.962 - ETA: 1s - loss: 1.960 - ETA: 1s - loss: 1.965 - ETA: 1s - loss: 1.962 - ETA: 1s - loss: 1.972 - ETA: 1s - loss: 1.970 - ETA: 1s - loss: 1.964 - ETA: 1s - loss: 1.963 - ETA: 1s - loss: 1.965 - ETA: 1s - loss: 1.964 - ETA: 1s - loss: 1.967 - ETA: 0s - loss: 1.971 - ETA: 0s - loss: 1.977 - ETA: 0s - loss: 1.978 - ETA: 0s - loss: 1.973 - ETA: 0s - loss: 1.973 - ETA: 0s - loss: 1.971 - ETA: 0s - loss: 1.973 - ETA: 0s - loss: 1.979 - ETA: 0s - loss: 1.980 - ETA: 0s - loss: 1.981 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.983 - 4s 39ms/step - loss: 1.9812\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 1.655 - ETA: 3s - loss: 1.797 - ETA: 3s - loss: 1.834 - ETA: 3s - loss: 1.819 - ETA: 3s - loss: 1.813 - ETA: 3s - loss: 1.813 - ETA: 3s - loss: 1.819 - ETA: 3s - loss: 1.843 - ETA: 3s - loss: 1.877 - ETA: 3s - loss: 1.880 - ETA: 3s - loss: 1.885 - ETA: 2s - loss: 1.878 - ETA: 2s - loss: 1.903 - ETA: 2s - loss: 1.889 - ETA: 2s - loss: 1.896 - ETA: 2s - loss: 1.898 - ETA: 2s - loss: 1.898 - ETA: 2s - loss: 1.896 - ETA: 2s - loss: 1.881 - ETA: 2s - loss: 1.890 - ETA: 2s - loss: 1.877 - ETA: 2s - loss: 1.877 - ETA: 2s - loss: 1.892 - ETA: 2s - loss: 1.888 - ETA: 1s - loss: 1.890 - ETA: 1s - loss: 1.886 - ETA: 1s - loss: 1.881 - ETA: 1s - loss: 1.886 - ETA: 1s - loss: 1.886 - ETA: 1s - loss: 1.894 - ETA: 1s - loss: 1.892 - ETA: 1s - loss: 1.892 - ETA: 1s - loss: 1.900 - ETA: 1s - loss: 1.900 - ETA: 1s - loss: 1.903 - ETA: 1s - loss: 1.905 - ETA: 1s - loss: 1.905 - ETA: 0s - loss: 1.906 - ETA: 0s - loss: 1.903 - ETA: 0s - loss: 1.904 - ETA: 0s - loss: 1.914 - ETA: 0s - loss: 1.920 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.932 - ETA: 0s - loss: 1.929 - ETA: 0s - loss: 1.930 - ETA: 0s - loss: 1.933 - ETA: 0s - loss: 1.932 - ETA: 0s - loss: 1.928 - 4s 39ms/step - loss: 1.9300\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 2.173 - ETA: 3s - loss: 2.073 - ETA: 3s - loss: 2.090 - ETA: 3s - loss: 2.044 - ETA: 3s - loss: 2.100 - ETA: 3s - loss: 2.059 - ETA: 3s - loss: 2.035 - ETA: 3s - loss: 1.997 - ETA: 3s - loss: 1.975 - ETA: 3s - loss: 1.949 - ETA: 3s - loss: 1.962 - ETA: 2s - loss: 1.974 - ETA: 2s - loss: 1.965 - ETA: 2s - loss: 1.944 - ETA: 2s - loss: 1.939 - ETA: 2s - loss: 1.937 - ETA: 2s - loss: 1.925 - ETA: 2s - loss: 1.948 - ETA: 2s - loss: 1.941 - ETA: 2s - loss: 1.947 - ETA: 2s - loss: 1.937 - ETA: 2s - loss: 1.940 - ETA: 2s - loss: 1.945 - ETA: 2s - loss: 1.948 - ETA: 1s - loss: 1.945 - ETA: 1s - loss: 1.951 - ETA: 1s - loss: 1.967 - ETA: 1s - loss: 1.957 - ETA: 1s - loss: 1.948 - ETA: 1s - loss: 1.941 - ETA: 1s - loss: 1.938 - ETA: 1s - loss: 1.936 - ETA: 1s - loss: 1.945 - ETA: 1s - loss: 1.948 - ETA: 1s - loss: 1.945 - ETA: 1s - loss: 1.955 - ETA: 1s - loss: 1.955 - ETA: 0s - loss: 1.947 - ETA: 0s - loss: 1.947 - ETA: 0s - loss: 1.946 - ETA: 0s - loss: 1.944 - ETA: 0s - loss: 1.942 - ETA: 0s - loss: 1.942 - ETA: 0s - loss: 1.941 - ETA: 0s - loss: 1.937 - ETA: 0s - loss: 1.940 - ETA: 0s - loss: 1.943 - ETA: 0s - loss: 1.940 - ETA: 0s - loss: 1.937 - ETA: 0s - loss: 1.940 - 4s 38ms/step - loss: 1.9365\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 2.345 - ETA: 3s - loss: 1.971 - ETA: 3s - loss: 1.906 - ETA: 3s - loss: 1.912 - ETA: 3s - loss: 1.963 - ETA: 3s - loss: 1.958 - ETA: 3s - loss: 1.973 - ETA: 3s - loss: 1.976 - ETA: 3s - loss: 1.953 - ETA: 3s - loss: 1.934 - ETA: 2s - loss: 1.936 - ETA: 2s - loss: 1.921 - ETA: 2s - loss: 1.907 - ETA: 2s - loss: 1.924 - ETA: 2s - loss: 1.937 - ETA: 2s - loss: 1.946 - ETA: 2s - loss: 1.947 - ETA: 2s - loss: 1.948 - ETA: 2s - loss: 1.956 - ETA: 2s - loss: 1.962 - ETA: 2s - loss: 1.958 - ETA: 2s - loss: 1.972 - ETA: 2s - loss: 1.985 - ETA: 1s - loss: 1.981 - ETA: 1s - loss: 1.978 - ETA: 1s - loss: 1.964 - ETA: 1s - loss: 1.965 - ETA: 1s - loss: 1.974 - ETA: 1s - loss: 1.972 - ETA: 1s - loss: 1.966 - ETA: 1s - loss: 1.971 - ETA: 1s - loss: 1.974 - ETA: 1s - loss: 1.971 - ETA: 1s - loss: 1.973 - ETA: 1s - loss: 1.985 - ETA: 1s - loss: 1.979 - ETA: 1s - loss: 1.982 - ETA: 0s - loss: 1.983 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.987 - ETA: 0s - loss: 1.992 - ETA: 0s - loss: 1.991 - ETA: 0s - loss: 1.991 - ETA: 0s - loss: 1.989 - ETA: 0s - loss: 1.989 - ETA: 0s - loss: 1.987 - ETA: 0s - loss: 1.993 - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 1.994 - ETA: 0s - loss: 1.992 - 4s 38ms/step - loss: 1.9941\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 1.788 - ETA: 3s - loss: 1.830 - ETA: 3s - loss: 1.853 - ETA: 3s - loss: 1.921 - ETA: 3s - loss: 1.907 - ETA: 3s - loss: 1.928 - ETA: 3s - loss: 1.953 - ETA: 3s - loss: 1.941 - ETA: 3s - loss: 1.918 - ETA: 3s - loss: 1.927 - ETA: 2s - loss: 1.966 - ETA: 2s - loss: 1.958 - ETA: 2s - loss: 1.948 - ETA: 2s - loss: 1.963 - ETA: 2s - loss: 1.947 - ETA: 2s - loss: 1.942 - ETA: 2s - loss: 1.949 - ETA: 2s - loss: 1.961 - ETA: 2s - loss: 1.952 - ETA: 2s - loss: 1.935 - ETA: 2s - loss: 1.933 - ETA: 2s - loss: 1.931 - ETA: 2s - loss: 1.933 - ETA: 1s - loss: 1.941 - ETA: 1s - loss: 1.947 - ETA: 1s - loss: 1.944 - ETA: 1s - loss: 1.943 - ETA: 1s - loss: 1.952 - ETA: 1s - loss: 1.952 - ETA: 1s - loss: 1.941 - ETA: 1s - loss: 1.948 - ETA: 1s - loss: 1.945 - ETA: 1s - loss: 1.942 - ETA: 1s - loss: 1.941 - ETA: 1s - loss: 1.934 - ETA: 1s - loss: 1.943 - ETA: 1s - loss: 1.948 - ETA: 0s - loss: 1.954 - ETA: 0s - loss: 1.945 - ETA: 0s - loss: 1.956 - ETA: 0s - loss: 1.952 - ETA: 0s - loss: 1.942 - ETA: 0s - loss: 1.937 - ETA: 0s - loss: 1.934 - ETA: 0s - loss: 1.930 - ETA: 0s - loss: 1.932 - ETA: 0s - loss: 1.934 - ETA: 0s - loss: 1.937 - ETA: 0s - loss: 1.938 - ETA: 0s - loss: 1.933 - 4s 38ms/step - loss: 1.9296\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 1.886 - ETA: 3s - loss: 2.132 - ETA: 3s - loss: 1.984 - ETA: 3s - loss: 1.985 - ETA: 3s - loss: 1.905 - ETA: 3s - loss: 1.858 - ETA: 3s - loss: 1.880 - ETA: 3s - loss: 1.911 - ETA: 3s - loss: 1.909 - ETA: 3s - loss: 1.920 - ETA: 2s - loss: 1.927 - ETA: 2s - loss: 1.955 - ETA: 2s - loss: 1.969 - ETA: 2s - loss: 1.957 - ETA: 2s - loss: 1.977 - ETA: 2s - loss: 1.977 - ETA: 2s - loss: 1.994 - ETA: 2s - loss: 1.983 - ETA: 2s - loss: 1.986 - ETA: 2s - loss: 1.982 - ETA: 2s - loss: 1.974 - ETA: 2s - loss: 1.971 - ETA: 2s - loss: 1.986 - ETA: 1s - loss: 1.983 - ETA: 1s - loss: 1.986 - ETA: 1s - loss: 1.983 - ETA: 1s - loss: 1.980 - ETA: 1s - loss: 1.981 - ETA: 1s - loss: 1.977 - ETA: 1s - loss: 1.971 - ETA: 1s - loss: 1.975 - ETA: 1s - loss: 1.962 - ETA: 1s - loss: 1.959 - ETA: 1s - loss: 1.952 - ETA: 1s - loss: 1.952 - ETA: 1s - loss: 1.951 - ETA: 1s - loss: 1.947 - ETA: 0s - loss: 1.942 - ETA: 0s - loss: 1.940 - ETA: 0s - loss: 1.945 - ETA: 0s - loss: 1.942 - ETA: 0s - loss: 1.938 - ETA: 0s - loss: 1.948 - ETA: 0s - loss: 1.945 - ETA: 0s - loss: 1.940 - ETA: 0s - loss: 1.937 - ETA: 0s - loss: 1.941 - ETA: 0s - loss: 1.944 - ETA: 0s - loss: 1.945 - ETA: 0s - loss: 1.945 - ETA: 0s - loss: 1.941 - 4s 38ms/step - loss: 1.9413\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 2.415 - ETA: 3s - loss: 2.036 - ETA: 3s - loss: 2.067 - ETA: 3s - loss: 1.977 - ETA: 3s - loss: 1.941 - ETA: 3s - loss: 1.990 - ETA: 3s - loss: 1.985 - ETA: 3s - loss: 1.974 - ETA: 3s - loss: 1.987 - ETA: 3s - loss: 2.043 - ETA: 3s - loss: 2.057 - ETA: 2s - loss: 2.067 - ETA: 2s - loss: 2.060 - ETA: 2s - loss: 2.049 - ETA: 2s - loss: 2.045 - ETA: 2s - loss: 2.029 - ETA: 2s - loss: 2.023 - ETA: 2s - loss: 2.021 - ETA: 2s - loss: 2.009 - ETA: 2s - loss: 2.007 - ETA: 2s - loss: 2.001 - ETA: 2s - loss: 2.002 - ETA: 2s - loss: 2.004 - ETA: 2s - loss: 1.990 - ETA: 1s - loss: 1.988 - ETA: 1s - loss: 1.988 - ETA: 1s - loss: 1.972 - ETA: 1s - loss: 1.972 - ETA: 1s - loss: 1.969 - ETA: 1s - loss: 1.972 - ETA: 1s - loss: 1.963 - ETA: 1s - loss: 1.961 - ETA: 1s - loss: 1.968 - ETA: 1s - loss: 1.969 - ETA: 1s - loss: 1.963 - ETA: 1s - loss: 1.965 - ETA: 1s - loss: 1.969 - ETA: 0s - loss: 1.969 - ETA: 0s - loss: 1.964 - ETA: 0s - loss: 1.961 - ETA: 0s - loss: 1.960 - ETA: 0s - loss: 1.965 - ETA: 0s - loss: 1.972 - ETA: 0s - loss: 1.972 - ETA: 0s - loss: 1.967 - ETA: 0s - loss: 1.973 - ETA: 0s - loss: 1.970 - ETA: 0s - loss: 1.971 - ETA: 0s - loss: 1.963 - ETA: 0s - loss: 1.966 - 4s 38ms/step - loss: 1.9664\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 1.950 - ETA: 3s - loss: 2.238 - ETA: 3s - loss: 2.233 - ETA: 3s - loss: 2.149 - ETA: 3s - loss: 2.150 - ETA: 3s - loss: 2.162 - ETA: 3s - loss: 2.100 - ETA: 3s - loss: 2.098 - ETA: 3s - loss: 2.087 - ETA: 3s - loss: 2.051 - ETA: 3s - loss: 2.078 - ETA: 3s - loss: 2.085 - ETA: 2s - loss: 2.087 - ETA: 2s - loss: 2.099 - ETA: 2s - loss: 2.102 - ETA: 2s - loss: 2.095 - ETA: 2s - loss: 2.084 - ETA: 2s - loss: 2.065 - ETA: 2s - loss: 2.046 - ETA: 2s - loss: 2.034 - ETA: 2s - loss: 2.038 - ETA: 2s - loss: 2.032 - ETA: 2s - loss: 2.033 - ETA: 2s - loss: 2.030 - ETA: 1s - loss: 2.024 - ETA: 1s - loss: 2.020 - ETA: 1s - loss: 2.016 - ETA: 1s - loss: 2.008 - ETA: 1s - loss: 1.995 - ETA: 1s - loss: 2.001 - ETA: 1s - loss: 2.000 - ETA: 1s - loss: 1.991 - ETA: 1s - loss: 1.988 - ETA: 1s - loss: 1.986 - ETA: 1s - loss: 1.994 - ETA: 1s - loss: 1.995 - ETA: 1s - loss: 1.991 - ETA: 0s - loss: 1.995 - ETA: 0s - loss: 1.999 - ETA: 0s - loss: 2.013 - ETA: 0s - loss: 2.006 - ETA: 0s - loss: 2.010 - ETA: 0s - loss: 2.006 - ETA: 0s - loss: 2.010 - ETA: 0s - loss: 2.010 - ETA: 0s - loss: 2.012 - ETA: 0s - loss: 2.010 - ETA: 0s - loss: 2.013 - ETA: 0s - loss: 2.011 - ETA: 0s - loss: 2.015 - 4s 38ms/step - loss: 2.0213\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 2.059 - ETA: 3s - loss: 1.945 - ETA: 3s - loss: 1.966 - ETA: 3s - loss: 1.915 - ETA: 3s - loss: 1.963 - ETA: 3s - loss: 1.957 - ETA: 3s - loss: 1.957 - ETA: 3s - loss: 1.985 - ETA: 3s - loss: 1.998 - ETA: 2s - loss: 2.005 - ETA: 2s - loss: 1.995 - ETA: 2s - loss: 1.996 - ETA: 2s - loss: 1.972 - ETA: 2s - loss: 1.978 - ETA: 2s - loss: 1.985 - ETA: 2s - loss: 1.967 - ETA: 2s - loss: 1.968 - ETA: 2s - loss: 1.961 - ETA: 2s - loss: 1.960 - ETA: 2s - loss: 1.966 - ETA: 2s - loss: 1.992 - ETA: 2s - loss: 1.991 - ETA: 1s - loss: 2.000 - ETA: 1s - loss: 2.005 - ETA: 1s - loss: 2.007 - ETA: 1s - loss: 2.002 - ETA: 1s - loss: 2.007 - ETA: 1s - loss: 2.007 - ETA: 1s - loss: 2.017 - ETA: 1s - loss: 2.013 - ETA: 1s - loss: 2.017 - ETA: 1s - loss: 2.020 - ETA: 1s - loss: 2.016 - ETA: 1s - loss: 2.014 - ETA: 1s - loss: 2.011 - ETA: 1s - loss: 2.003 - ETA: 0s - loss: 2.003 - ETA: 0s - loss: 2.000 - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 1.994 - ETA: 0s - loss: 1.989 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.987 - ETA: 0s - loss: 1.984 - ETA: 0s - loss: 1.978 - ETA: 0s - loss: 1.980 - ETA: 0s - loss: 1.981 - ETA: 0s - loss: 1.975 - ETA: 0s - loss: 1.974 - ETA: 0s - loss: 1.977 - 4s 37ms/step - loss: 1.9783\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 1.777 - ETA: 3s - loss: 1.922 - ETA: 3s - loss: 1.924 - ETA: 3s - loss: 1.980 - ETA: 3s - loss: 2.003 - ETA: 3s - loss: 1.960 - ETA: 3s - loss: 1.976 - ETA: 3s - loss: 1.974 - ETA: 3s - loss: 1.951 - ETA: 3s - loss: 1.943 - ETA: 3s - loss: 1.943 - ETA: 2s - loss: 1.948 - ETA: 2s - loss: 1.949 - ETA: 2s - loss: 1.952 - ETA: 2s - loss: 1.945 - ETA: 2s - loss: 1.947 - ETA: 2s - loss: 1.950 - ETA: 2s - loss: 1.960 - ETA: 2s - loss: 1.951 - ETA: 2s - loss: 1.952 - ETA: 2s - loss: 1.954 - ETA: 2s - loss: 1.956 - ETA: 2s - loss: 1.954 - ETA: 1s - loss: 1.957 - ETA: 1s - loss: 1.969 - ETA: 1s - loss: 1.966 - ETA: 1s - loss: 1.962 - ETA: 1s - loss: 1.959 - ETA: 1s - loss: 1.974 - ETA: 1s - loss: 1.978 - ETA: 1s - loss: 1.974 - ETA: 1s - loss: 1.979 - ETA: 1s - loss: 1.979 - ETA: 1s - loss: 1.970 - ETA: 1s - loss: 1.962 - ETA: 1s - loss: 1.961 - ETA: 1s - loss: 1.967 - ETA: 0s - loss: 1.974 - ETA: 0s - loss: 1.978 - ETA: 0s - loss: 1.973 - ETA: 0s - loss: 1.968 - ETA: 0s - loss: 1.963 - ETA: 0s - loss: 1.960 - ETA: 0s - loss: 1.956 - ETA: 0s - loss: 1.957 - ETA: 0s - loss: 1.956 - ETA: 0s - loss: 1.961 - ETA: 0s - loss: 1.963 - ETA: 0s - loss: 1.955 - ETA: 0s - loss: 1.958 - 4s 38ms/step - loss: 1.9563\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 1.842 - ETA: 3s - loss: 1.874 - ETA: 3s - loss: 1.884 - ETA: 3s - loss: 1.908 - ETA: 3s - loss: 1.922 - ETA: 3s - loss: 1.994 - ETA: 3s - loss: 2.070 - ETA: 3s - loss: 2.067 - ETA: 3s - loss: 2.051 - ETA: 3s - loss: 2.023 - ETA: 3s - loss: 2.034 - ETA: 2s - loss: 2.016 - ETA: 2s - loss: 2.001 - ETA: 2s - loss: 2.005 - ETA: 2s - loss: 1.994 - ETA: 2s - loss: 1.999 - ETA: 2s - loss: 2.003 - ETA: 2s - loss: 1.987 - ETA: 2s - loss: 1.994 - ETA: 2s - loss: 1.997 - ETA: 2s - loss: 2.002 - ETA: 2s - loss: 1.991 - ETA: 2s - loss: 1.984 - ETA: 2s - loss: 1.996 - ETA: 1s - loss: 2.003 - ETA: 1s - loss: 1.995 - ETA: 1s - loss: 1.998 - ETA: 1s - loss: 2.006 - ETA: 1s - loss: 2.012 - ETA: 1s - loss: 2.018 - ETA: 1s - loss: 2.013 - ETA: 1s - loss: 2.004 - ETA: 1s - loss: 1.998 - ETA: 1s - loss: 2.005 - ETA: 1s - loss: 2.004 - ETA: 1s - loss: 1.995 - ETA: 1s - loss: 1.994 - ETA: 0s - loss: 1.996 - ETA: 0s - loss: 1.997 - ETA: 0s - loss: 1.997 - ETA: 0s - loss: 1.989 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.983 - ETA: 0s - loss: 1.990 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.989 - ETA: 0s - loss: 1.989 - ETA: 0s - loss: 1.996 - ETA: 0s - loss: 1.994 - ETA: 0s - loss: 1.990 - 4s 40ms/step - loss: 1.9904\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 1.556 - ETA: 3s - loss: 1.717 - ETA: 3s - loss: 1.933 - ETA: 3s - loss: 1.979 - ETA: 3s - loss: 1.964 - ETA: 3s - loss: 1.984 - ETA: 3s - loss: 1.936 - ETA: 3s - loss: 1.911 - ETA: 3s - loss: 1.962 - ETA: 3s - loss: 1.933 - ETA: 3s - loss: 1.948 - ETA: 3s - loss: 1.957 - ETA: 2s - loss: 1.965 - ETA: 2s - loss: 1.951 - ETA: 2s - loss: 1.948 - ETA: 2s - loss: 1.956 - ETA: 2s - loss: 1.953 - ETA: 2s - loss: 1.954 - ETA: 2s - loss: 1.972 - ETA: 2s - loss: 1.972 - ETA: 2s - loss: 1.975 - ETA: 2s - loss: 1.974 - ETA: 2s - loss: 1.961 - ETA: 2s - loss: 1.959 - ETA: 2s - loss: 1.959 - ETA: 1s - loss: 1.964 - ETA: 1s - loss: 1.956 - ETA: 1s - loss: 1.957 - ETA: 1s - loss: 1.968 - ETA: 1s - loss: 1.970 - ETA: 1s - loss: 1.972 - ETA: 1s - loss: 1.970 - ETA: 1s - loss: 1.979 - ETA: 1s - loss: 1.990 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.988 - ETA: 1s - loss: 1.986 - ETA: 0s - loss: 1.982 - ETA: 0s - loss: 1.977 - ETA: 0s - loss: 1.975 - ETA: 0s - loss: 1.980 - ETA: 0s - loss: 1.982 - ETA: 0s - loss: 1.982 - ETA: 0s - loss: 1.979 - ETA: 0s - loss: 1.976 - ETA: 0s - loss: 1.977 - ETA: 0s - loss: 1.972 - ETA: 0s - loss: 1.972 - ETA: 0s - loss: 1.972 - ETA: 0s - loss: 1.970 - 4s 40ms/step - loss: 1.9683\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 2.069 - ETA: 3s - loss: 2.021 - ETA: 3s - loss: 1.920 - ETA: 3s - loss: 1.991 - ETA: 3s - loss: 2.049 - ETA: 3s - loss: 2.016 - ETA: 3s - loss: 2.055 - ETA: 3s - loss: 2.036 - ETA: 2s - loss: 2.011 - ETA: 2s - loss: 2.000 - ETA: 2s - loss: 1.996 - ETA: 2s - loss: 2.008 - ETA: 2s - loss: 2.013 - ETA: 2s - loss: 1.994 - ETA: 2s - loss: 1.980 - ETA: 2s - loss: 1.967 - ETA: 2s - loss: 1.972 - ETA: 2s - loss: 1.973 - ETA: 2s - loss: 1.962 - ETA: 2s - loss: 1.956 - ETA: 2s - loss: 1.953 - ETA: 2s - loss: 1.943 - ETA: 1s - loss: 1.938 - ETA: 1s - loss: 1.936 - ETA: 1s - loss: 1.934 - ETA: 1s - loss: 1.931 - ETA: 1s - loss: 1.931 - ETA: 1s - loss: 1.932 - ETA: 1s - loss: 1.942 - ETA: 1s - loss: 1.948 - ETA: 1s - loss: 1.952 - ETA: 1s - loss: 1.956 - ETA: 1s - loss: 1.955 - ETA: 1s - loss: 1.962 - ETA: 1s - loss: 1.970 - ETA: 1s - loss: 1.964 - ETA: 0s - loss: 1.962 - ETA: 0s - loss: 1.959 - ETA: 0s - loss: 1.959 - ETA: 0s - loss: 1.957 - ETA: 0s - loss: 1.964 - ETA: 0s - loss: 1.960 - ETA: 0s - loss: 1.955 - ETA: 0s - loss: 1.958 - ETA: 0s - loss: 1.953 - ETA: 0s - loss: 1.956 - ETA: 0s - loss: 1.952 - ETA: 0s - loss: 1.956 - ETA: 0s - loss: 1.960 - ETA: 0s - loss: 1.963 - 4s 37ms/step - loss: 1.9652\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - ETA: 3s - loss: 2.038 - ETA: 3s - loss: 2.107 - ETA: 3s - loss: 2.082 - ETA: 3s - loss: 1.930 - ETA: 3s - loss: 1.918 - ETA: 3s - loss: 1.949 - ETA: 3s - loss: 1.919 - ETA: 3s - loss: 1.921 - ETA: 3s - loss: 1.926 - ETA: 3s - loss: 1.953 - ETA: 3s - loss: 1.937 - ETA: 2s - loss: 1.965 - ETA: 2s - loss: 1.967 - ETA: 2s - loss: 1.975 - ETA: 2s - loss: 1.974 - ETA: 2s - loss: 1.974 - ETA: 2s - loss: 1.968 - ETA: 2s - loss: 1.988 - ETA: 2s - loss: 1.994 - ETA: 2s - loss: 1.996 - ETA: 2s - loss: 2.002 - ETA: 2s - loss: 1.999 - ETA: 2s - loss: 2.002 - ETA: 2s - loss: 1.997 - ETA: 1s - loss: 1.996 - ETA: 1s - loss: 1.988 - ETA: 1s - loss: 1.990 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.990 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.985 - ETA: 1s - loss: 1.989 - ETA: 1s - loss: 1.982 - ETA: 1s - loss: 1.994 - ETA: 1s - loss: 1.995 - ETA: 1s - loss: 1.999 - ETA: 1s - loss: 2.008 - ETA: 0s - loss: 2.006 - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 2.000 - ETA: 0s - loss: 2.000 - ETA: 0s - loss: 1.992 - ETA: 0s - loss: 1.997 - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 1.998 - ETA: 0s - loss: 1.999 - ETA: 0s - loss: 1.997 - ETA: 0s - loss: 2.000 - ETA: 0s - loss: 2.001 - 4s 39ms/step - loss: 2.0005\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - ETA: 4s - loss: 1.937 - ETA: 3s - loss: 1.970 - ETA: 3s - loss: 1.839 - ETA: 3s - loss: 1.880 - ETA: 3s - loss: 1.912 - ETA: 3s - loss: 1.976 - ETA: 3s - loss: 1.955 - ETA: 3s - loss: 1.929 - ETA: 3s - loss: 1.935 - ETA: 3s - loss: 1.945 - ETA: 3s - loss: 1.950 - ETA: 2s - loss: 1.947 - ETA: 2s - loss: 1.934 - ETA: 2s - loss: 1.941 - ETA: 2s - loss: 1.945 - ETA: 2s - loss: 1.945 - ETA: 2s - loss: 1.933 - ETA: 2s - loss: 1.916 - ETA: 2s - loss: 1.907 - ETA: 2s - loss: 1.903 - ETA: 2s - loss: 1.911 - ETA: 2s - loss: 1.904 - ETA: 2s - loss: 1.911 - ETA: 2s - loss: 1.904 - ETA: 1s - loss: 1.909 - ETA: 1s - loss: 1.918 - ETA: 1s - loss: 1.928 - ETA: 1s - loss: 1.932 - ETA: 1s - loss: 1.928 - ETA: 1s - loss: 1.930 - ETA: 1s - loss: 1.930 - ETA: 1s - loss: 1.935 - ETA: 1s - loss: 1.937 - ETA: 1s - loss: 1.933 - ETA: 1s - loss: 1.939 - ETA: 1s - loss: 1.937 - ETA: 1s - loss: 1.941 - ETA: 0s - loss: 1.936 - ETA: 0s - loss: 1.933 - ETA: 0s - loss: 1.931 - ETA: 0s - loss: 1.931 - ETA: 0s - loss: 1.931 - ETA: 0s - loss: 1.931 - ETA: 0s - loss: 1.932 - ETA: 0s - loss: 1.928 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.925 - ETA: 0s - loss: 1.929 - ETA: 0s - loss: 1.930 - ETA: 0s - loss: 1.934 - 4s 38ms/step - loss: 1.9350\n",
      "Epoch 21/200\n",
      " 23/100 [=====>........................] - ETA: 4s - loss: 2.064 - ETA: 3s - loss: 2.052 - ETA: 3s - loss: 1.981 - ETA: 3s - loss: 1.930 - ETA: 3s - loss: 1.935 - ETA: 3s - loss: 1.999 - ETA: 3s - loss: 2.034 - ETA: 3s - loss: 2.034 - ETA: 3s - loss: 1.982 - ETA: 3s - loss: 1.978 - ETA: 3s - loss: 1.985 - ETA: 3s - loss: 1.9871"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m, in \u001b[0;32mrun_code\u001b[0m:\nLine \u001b[0;34m3437\u001b[0m:  exec(code_obj, \u001b[36mself\u001b[39;49;00m.user_global_ns, \u001b[36mself\u001b[39;49;00m.user_ns)\n",
      "In  \u001b[0;34m[5]\u001b[0m:\nLine \u001b[0;34m6\u001b[0m:     result = nn_integrator.model.fit(train_batches,epochs=EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m, in \u001b[0;32merror_handler\u001b[0m:\nLine \u001b[0;34m64\u001b[0m:    \u001b[34mreturn\u001b[39;49;00m fn(*args, **kwargs)\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m, in \u001b[0;32mfit\u001b[0m:\nLine \u001b[0;34m1216\u001b[0m:  tmp_logs = \u001b[36mself\u001b[39;49;00m.train_function(iterator)\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m, in \u001b[0;32merror_handler\u001b[0m:\nLine \u001b[0;34m150\u001b[0m:   \u001b[34mreturn\u001b[39;49;00m fn(*args, **kwargs)\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m, in \u001b[0;32m__call__\u001b[0m:\nLine \u001b[0;34m910\u001b[0m:   result = \u001b[36mself\u001b[39;49;00m._call(*args, **kwds)\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m, in \u001b[0;32m_call\u001b[0m:\nLine \u001b[0;34m942\u001b[0m:   \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._stateless_fn(*args, **kwds)  \u001b[37m# pylint: disable=not-callable\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m, in \u001b[0;32m__call__\u001b[0m:\nLine \u001b[0;34m3130\u001b[0m:  \u001b[34mreturn\u001b[39;49;00m graph_function._call_flat(\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m, in \u001b[0;32m_call_flat\u001b[0m:\nLine \u001b[0;34m1959\u001b[0m:  \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._build_call_outputs(\u001b[36mself\u001b[39;49;00m._inference_function.call(\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m, in \u001b[0;32mcall\u001b[0m:\nLine \u001b[0;34m598\u001b[0m:   outputs = execute.execute(\n",
      "File \u001b[0;34m/home/eike/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m, in \u001b[0;32mquick_execute\u001b[0m:\nLine \u001b[0;34m58\u001b[0m:    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "EPOCHS=200\n",
    "STEPS_PER_EPOCH=100\n",
    "log_dir = './tb_logs/'\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "result = nn_integrator.model.fit(train_batches,epochs=EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                 callbacks=tensorboard_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-support",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "![Loss function](./loss_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-nicaragua",
   "metadata": {},
   "source": [
    "## Plot trajectories generated by Verlet integrator and neural network based integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final time\n",
    "T_final = 32\n",
    "\n",
    "# Initial conditions\n",
    "x = np.zeros(1)\n",
    "v = np.zeros(1)\n",
    "x[0] = 1.0\n",
    "v[0] = 0.0\n",
    "verlet_integrator.set_state(x,v)\n",
    "\n",
    "# ==== Verlet integrator ====\n",
    "t = 0.0\n",
    "t_verlet = []\n",
    "x_verlet = []\n",
    "E_verlet = []\n",
    "while t<T_final:\n",
    "    t_verlet.append(t)\n",
    "    E_verlet.append(verlet_integrator.energy())\n",
    "    x_verlet.append(verlet_integrator.x[0])\n",
    "    verlet_integrator.integrate(1)\n",
    "    t += dt_verlet\n",
    "        \n",
    "\n",
    "# ==== Neural network integrator ====\n",
    "\n",
    "# Initialise with Verlet integrator\n",
    "x_initial = np.zeros((nn_integrator.nsteps,nn_integrator.dynamical_system.dim))\n",
    "v_initial = np.zeros((nn_integrator.nsteps,nn_integrator.dynamical_system.dim))\n",
    "verlet_integrator.set_state(x,v)\n",
    "for k in range(nn_integrator.nsteps):    \n",
    "    x_initial[k,:] = verlet_integrator.x[:]\n",
    "    v_initial[k,:] = verlet_integrator.v[:]\n",
    "    verlet_integrator.integrate(int(dt/dt_verlet))\n",
    "nn_integrator.set_state(x_initial,v_initial)\n",
    "t = (nn_integrator.nsteps-1)*nn_integrator.dt\n",
    "\n",
    "# Timestepping loop\n",
    "t_nn = []\n",
    "x_nn = []\n",
    "E_nn = []\n",
    "while t<T_final:\n",
    "    t_nn.append(t)\n",
    "    x_nn.append(nn_integrator.x[0])\n",
    "    E_nn.append(nn_integrator.energy())\n",
    "    nn_integrator.integrate(1)\n",
    "    t += dt\n",
    "\n",
    "# Plot position as a function of time\n",
    "plt.plot(t_verlet,x_verlet,label='Verlet',color='blue')\n",
    "plt.plot(t_nn,x_nn,label='Neural network',color='red')\n",
    "plt.legend(loc='lower right')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('time $t$')\n",
    "ax.set_ylabel('position $x(t)$')\n",
    "ax.set_title('Position')\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "# Plot energy as a function of time\n",
    "# (subtract energy at time t=0 to show energy drift)\n",
    "fig, axs = plt.subplots(2,1)\n",
    "axs[0].plot(t_verlet,E_verlet-E_verlet[0],label='Verlet',color='blue')\n",
    "axs[1].plot(t_nn,E_nn-E_nn[0],label='Neural network',color='red')\n",
    "plt.legend(loc='lower right')\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('time $t$')\n",
    "    ax.set_ylabel('energy shift $E(t)-E(t_0)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "altered-brick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot potential and kinetic energy (for debugging)\n",
    "X = tf.expand_dims(tf.constant(np.arange(-2.0,2.0,0.01)),axis=1)\n",
    "\n",
    "V_pot = nn_integrator.model.V_pot(X).numpy().flatten()\n",
    "T_kin = nn_integrator.model.T_kin(X).numpy().flatten()\n",
    "plt.plot(X,V_pot,linewidth=2,color='blue',label='V(q)')\n",
    "plt.plot(X,T_kin,linewidth=2,color='red',label='T(p)')\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('q / p')\n",
    "ax.set_ylabel('energy')\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a7366-a4d2-430d-9229-3ff77ab13244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
